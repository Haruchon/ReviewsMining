{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fase 0: Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/alulab/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Reviews.csv\",nrows=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs=[]\n",
    "for index, row in df.iterrows():\n",
    "    docs.append(row['Summary']+\" \"+ row['Text'])\n",
    "docs=np.array(docs)\n",
    "y=np.array(df['Score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fase 1: Limpieza de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tokens(text):\n",
    "    text=text.lower()\n",
    "    text=\" \".join([word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)])\n",
    "    stopwords_eng = nltk.corpus.stopwords.words('english')\n",
    "    text=\" \".join([token for token in text.split() if token not in stopwords_eng])\n",
    "    text=\"\".join([ch for ch in text if ch not in list(string.punctuation)])\n",
    "    text=\" \".join([token for token in text.split()])\n",
    "    \n",
    "    wnl = nltk.WordNetLemmatizer()\n",
    "    text=\" \".join([wnl.lemmatize(token) for token in text.split()])\n",
    "    text=\" \".join([token for token in text.split() if (len(token)>2) & (len(token)<20)])\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Delight\" says it all This is a confection that has been around a few centuries.  It is a light, pillowy citrus gelatin with nuts - in this case Filberts. And it is cut into tiny squares and then liberally coated with powdered sugar.  And it is a tiny mouthful of heaven.  Not too chewy, and very flavorful.  I highly recommend this yummy treat.  If you are familiar with the story of C.S. Lewis' \"The Lion, The Witch, and The Wardrobe\" - this is the treat that seduces Edmund into selling out his Brother and Sisters to the Witch.\n",
      "['delight', 'say', 'confection', 'around', 'century', 'light', 'pillowy', 'citrus', 'gelatin', 'nut', 'case', 'filbert', 'cut', 'tiny', 'square', 'liberally', 'coated', 'powdered', 'sugar', 'tiny', 'mouthful', 'heaven', 'chewy', 'flavorful', 'highly', 'recommend', 'yummy', 'treat', 'familiar', 'story', 'lewis', 'lion', 'witch', 'wardrobe', 'treat', 'seduces', 'edmund', 'selling', 'brother', 'sister', 'witch']\n"
     ]
    }
   ],
   "source": [
    "i=2\n",
    "print(docs[i])\n",
    "print(clean_tokens(docs[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fase 2: Vectorizar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 18912)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_fidf = TfidfVectorizer(tokenizer=clean_tokens, stop_words='english').fit_transform(docs)\n",
    "X_fidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_clean=[]\n",
    "for i in docs:\n",
    "    docs_clean.append(\" \".join(clean_tokens(i)))\n",
    "    \n",
    "docs_clean=np.array(docs_clean)\n",
    "docs_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 19145)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X_bag = vectorizer.fit_transform(docs_clean)\n",
    "X_bag.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fase 3: Entrenar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Ejemplos de algoritmos que pueden probar (pueden usar otros si desean)\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_model(clf, X, y):\n",
    "    scores = cross_val_score(clf, X, y, cv=5)\n",
    "    print(\"%s accuracy: %0.2f (+/- %0.2f)\" % \\\n",
    "          (str(clf.__class__).split('.')[-1].replace('>','').replace(\"'\",''), \n",
    "          scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_models(X, y):\n",
    "    run_model(LinearSVC(), X, y)\n",
    "    run_model(SGDClassifier(), X, y)\n",
    "    run_model(Perceptron(), X, y)\n",
    "    run_model(PassiveAggressiveClassifier(), X, y)\n",
    "    run_model(BernoulliNB(), X, y)\n",
    "    run_model(MultinomialNB(), X, y)\n",
    "    run_model(KNeighborsClassifier(), X, y)\n",
    "    run_model(NearestCentroid(), X, y)\n",
    "    run_model(RandomForestClassifier(n_estimators=100, max_depth=10), X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 18912) (10000,)\n",
      "(10000, 19145) (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_fidf.shape,y.shape)\n",
    "#print(X_spacy.shape,y.shape)\n",
    "print(X_bag.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC accuracy: 0.67 (+/- 0.01)\n",
      "SGDClassifier accuracy: 0.68 (+/- 0.02)\n",
      "Perceptron accuracy: 0.61 (+/- 0.03)\n",
      "PassiveAggressiveClassifier accuracy: 0.64 (+/- 0.02)\n",
      "BernoulliNB accuracy: 0.61 (+/- 0.01)\n",
      "MultinomialNB accuracy: 0.62 (+/- 0.00)\n",
      "KNeighborsClassifier accuracy: 0.57 (+/- 0.03)\n",
      "NearestCentroid accuracy: 0.56 (+/- 0.08)\n",
      "RandomForestClassifier accuracy: 0.62 (+/- 0.00)\n"
     ]
    }
   ],
   "source": [
    "run_models(X_fidf, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC accuracy: 0.62 (+/- 0.03)\n",
      "SGDClassifier accuracy: 0.63 (+/- 0.03)\n",
      "Perceptron accuracy: 0.61 (+/- 0.02)\n",
      "PassiveAggressiveClassifier accuracy: 0.64 (+/- 0.02)\n",
      "BernoulliNB accuracy: 0.61 (+/- 0.02)\n",
      "MultinomialNB accuracy: 0.64 (+/- 0.02)\n",
      "KNeighborsClassifier accuracy: 0.59 (+/- 0.02)\n",
      "NearestCentroid accuracy: 0.54 (+/- 0.05)\n",
      "RandomForestClassifier accuracy: 0.62 (+/- 0.00)\n"
     ]
    }
   ],
   "source": [
    "run_models(X_bag, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
